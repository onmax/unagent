---
icon: i-lucide-layers
---
# context

> Token estimation and context management for LLM prompts.

## Usage
```ts
import { buildContext, estimateTokens, splitByTokens, truncateToFit } from 'unagent/context'
```

## API

### Token Estimation
#### `estimateTokens(text, options?)`

Estimate token count using character-based approximation (~4 chars per token).

```ts
const tokens = estimateTokens('Hello, world!')
// â†’ 4
```

With custom ratio:

```ts
const tokens = estimateTokens(text, { charsPerToken: 3 })
```

### Truncation
#### `truncateToFit(content, options)`

Truncate content to fit within token budget.

```ts
const truncated = truncateToFit(longText, {
  maxTokens: 1000,
  suffix: '\n...[truncated]', // default
})
```

Options:

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `maxTokens` | `number` | required | Maximum tokens allowed |
| `charsPerToken` | `number` | `4` | Characters per token ratio |
| `suffix` | `string` | `'\n...[truncated]'` | Suffix added when truncated |

### Context Building
#### `buildContext(items, options)`

Assemble context from multiple items within a token budget, prioritizing by importance.

```ts
const result = buildContext([
  { content: systemPrompt, priority: 100, id: 'system' },
  { content: userMessage, priority: 90, id: 'user' },
  { content: fileContents, priority: 50, id: 'file' },
  { content: history, priority: 10, id: 'history' },
], {
  maxTokens: 4000,
  separator: '\n\n',
})

console.log(result.content) // assembled context
console.log(result.tokens) // estimated token count
console.log(result.included) // ['system', 'user', 'file']
console.log(result.excluded) // ['history']
```

Items with higher `priority` are included first. Items that don't fit are excluded.

### Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `maxTokens` | `number` | required | Token budget |
| `charsPerToken` | `number` | `4` | Characters per token |
| `separator` | `string` | `'\n\n'` | Separator between items |

### Result

```ts
interface BuildContextResult {
  content: string // assembled context
  tokens: number // estimated token count
  included: string[] // IDs of included items
  excluded: string[] // IDs of excluded items
}
```

### Chunking
#### `splitByTokens(text, maxTokensPerChunk, options?)`

Split text into chunks by token count, preferring to split at newlines.

```ts
const chunks = splitByTokens(largeDocument, 2000)

for (const chunk of chunks) {
  await processChunk(chunk)
}
```

### Types
```ts
interface TokenEstimateOptions {
  charsPerToken?: number
}

interface TruncateOptions {
  maxTokens: number
  charsPerToken?: number
  suffix?: string
}

interface ContextItem {
  content: string
  priority?: number // higher = more important
  id?: string // identifier for tracking
}

interface BuildContextOptions {
  maxTokens: number
  charsPerToken?: number
  separator?: string
}
```

## Examples

### Example: Building Agent Context
```ts
import { buildContext, truncateToFit } from 'unagent/context'

function prepareAgentContext(params: {
  systemPrompt: string
  userMessage: string
  files: Array<{ path: string, content: string }>
  maxTokens: number
}) {
  const items = [
    { content: params.systemPrompt, priority: 100, id: 'system' },
    { content: params.userMessage, priority: 90, id: 'user' },
    ...params.files.map((f, i) => ({
      content: `## ${f.path}\n${truncateToFit(f.content, { maxTokens: 500 })}`,
      priority: 50 - i, // earlier files get higher priority
      id: f.path,
    })),
  ]

  return buildContext(items, { maxTokens: params.maxTokens })
}
```

## Related
- [All Modules](/modules)
